{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking1 当我们思考数据源的时候，都有哪些维度，如果你想要使用爬虫抓取数据，都有哪些工具  \n",
    "数据维度：公开数据源；爬虫抓取；传感器数据；日志采集；    \n",
    "爬虫方式：python脚本，第三方工具（八爪鱼，后羿）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking2 企业的数据源来自多个维度，请思考什么是企业的数据中台，你是如何理解一方数据，二方数据，三方数据?  \n",
    "数据中台为解决多个业务分支系统数据无法共享，贯通的问题，将各个业务体系的数据融合在一起，扩展为中台，统一对各分支进行支持。  \n",
    "第一方数据是指产品内的线上生产数据；第二方数据是指媒体交互行为数据；第三方数据是数据供应商可提供数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# action1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.162277660187101"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = 1e-10\n",
    "def sqr10():\n",
    "    low,high = 3,4\n",
    "    mid = (low + high)/2\n",
    "    while high-low > e:\n",
    "        if mid *mid>10:\n",
    "            high=mid\n",
    "        else:\n",
    "            low=mid\n",
    "        mid = (high+low)/2\n",
    "    return mid\n",
    "sqr10()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# action2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xinqi\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:179: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
      "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.12365auto.com/zlts/0-0-0-0-0-0_0-01.shtml\n",
      "http://www.12365auto.com/zlts/0-0-0-0-0-0_0-02.shtml\n",
      "http://www.12365auto.com/zlts/0-0-0-0-0-0_0-03.shtml\n",
      "http://www.12365auto.com/zlts/0-0-0-0-0-0_0-04.shtml\n",
      "http://www.12365auto.com/zlts/0-0-0-0-0-0_0-05.shtml\n",
      "http://www.12365auto.com/zlts/0-0-0-0-0-0_0-06.shtml\n",
      "http://www.12365auto.com/zlts/0-0-0-0-0-0_0-07.shtml\n",
      "http://www.12365auto.com/zlts/0-0-0-0-0-0_0-08.shtml\n",
      "http://www.12365auto.com/zlts/0-0-0-0-0-0_0-09.shtml\n",
      "http://www.12365auto.com/zlts/0-0-0-0-0-0_0-010.shtml\n",
      "http://www.12365auto.com/zlts/0-0-0-0-0-0_0-011.shtml\n",
      "http://www.12365auto.com/zlts/0-0-0-0-0-0_0-012.shtml\n",
      "http://www.12365auto.com/zlts/0-0-0-0-0-0_0-013.shtml\n",
      "http://www.12365auto.com/zlts/0-0-0-0-0-0_0-014.shtml\n",
      "http://www.12365auto.com/zlts/0-0-0-0-0-0_0-015.shtml\n",
      "http://www.12365auto.com/zlts/0-0-0-0-0-0_0-016.shtml\n",
      "http://www.12365auto.com/zlts/0-0-0-0-0-0_0-017.shtml\n",
      "http://www.12365auto.com/zlts/0-0-0-0-0-0_0-018.shtml\n",
      "http://www.12365auto.com/zlts/0-0-0-0-0-0_0-019.shtml\n",
      "http://www.12365auto.com/zlts/0-0-0-0-0-0_0-020.shtml\n"
     ]
    }
   ],
   "source": [
    "# 使用request + BeautifulSoup提取12365auto投诉信息\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# 根据request_url得到soup\n",
    "def get_page_content(request_url):\n",
    "    # 得到页面的内容\n",
    "    headers={'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36'}\n",
    "    html=requests.get(request_url,headers=headers,timeout=10)\n",
    "    content = html.text\n",
    "    #print(content)\n",
    "\n",
    "    # 通过content创建BeautifulSoup对象\n",
    "    soup = BeautifulSoup(content, 'html.parser', from_encoding='utf-8')\n",
    "    return soup\n",
    "\n",
    "# 分析当前页面的投诉\n",
    "def analysis(soup):\n",
    "    # 找到完整的投诉信息框\n",
    "    temp = soup.find('div',class_=\"tslb_b\")\n",
    "    # 创建DataFrame\n",
    "    df = pd.DataFrame(columns = ['投诉编号', '投诉品牌', '投诉车系', '投诉车型', '问题简述', '典型问题', '投诉时间', '投诉状态'])\n",
    "    tr_list = temp.find_all('tr')\n",
    "    for tr in tr_list:\n",
    "        # ToDo：提取汽车投诉\n",
    "        temp ={}\n",
    "        td_list = tr.find_all('td')\n",
    "        if len(td_list)>0:\n",
    "            id,brand,car_model,type,desc,problem,datetime,status = td_list[0].text, td_list[1].text, td_list[2].text, td_list[3].text, td_list[4].text, td_list[5].text, td_list[6].text, td_list[7].text\n",
    "            temp['投诉编号'],temp['投诉品牌'],temp['投诉车系'],temp['投诉车型'],temp['问题简述'],temp['典型问题'],temp['投诉时间'],temp['投诉状态'] =id,brand,car_model,type,desc,problem,datetime,status\n",
    "            df = df.append(temp,ignore_index=True)\n",
    "    return df\n",
    "\n",
    "page_num = 20\n",
    "base_url = 'http://www.12365auto.com/zlts/0-0-0-0-0-0_0-0'\n",
    "\n",
    "result = pd.DataFrame(columns = ['投诉编号', '投诉品牌', '投诉车系', '投诉车型', '问题简述', '典型问题', '投诉时间', '投诉状态'])\n",
    "for i in range(page_num):\n",
    "    request_url = base_url+str(i+1)+'.shtml'\n",
    "    soup = get_page_content(request_url)\n",
    "    print(request_url)\n",
    "    df = analysis(soup)\n",
    "    #print(df)\n",
    "    result = result.append(df)\n",
    "    \n",
    "result.to_csv('car_complain.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
